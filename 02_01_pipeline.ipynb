{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85335eb0-fc3a-4bf0-90ba-13078e379182",
   "metadata": {},
   "source": [
    "# HF Transformers Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63756c4-7c05-4264-a9da-5abebf935b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anubhavmaity/mambaforge/envs/transformers-course/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc292b31-1c1e-480f-b70e-5c3af7dc5b71",
   "metadata": {},
   "source": [
    "## Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671d8621-3bf7-4290-8206-6d218509037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82dffa4-b73e-4a48-9867-5ff3b1fd083e",
   "metadata": {},
   "source": [
    "Here, we can see that the model used is `distilbert-base-uncased-finetuned-sst-2-english`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2cb8c8-9eae-4059-bc9c-76cba731a17c",
   "metadata": {},
   "source": [
    "Lets use a IMDB review for the moview Borat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954a3ea6-860d-4aa2-baae-ad1afd51a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = '''This movie is not for the faint of heart or the easily offended. \n",
    "It is a brilliant and scathing critique of the American society and culture, disguised as a comedy.\n",
    "The movie exposes the hypocrisy, greed, violence, racism, and ignorance that plague the nation, through the absurd and hilarious adventures of Borat Sagdiyev, a Kazakh journalist who travels to the US to make a documentary. \n",
    "The movie is full of shocking and outrageous scenes that will make you laugh, cringe, and question your own beliefs and values. \n",
    "The movie is not meant to be taken literally or seriously, but rather as a mirror that reflects the ugly truth about ourselves. \n",
    "The movie is a masterpiece of irony and satire, and one of the most original and daring comedies ever made.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "002f971e-499a-4173-94c6-ceb69951b44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9995748400688171}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306515e8-ac96-4d42-a91e-3da595a0fddf",
   "metadata": {},
   "source": [
    "Lets use a neutral review and see the models classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781d1542-fa41-4788-92f3-8f76ad7ba948",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_review = '''\n",
    "\"A Rollercoaster of Emotions\" is a visually stunning masterpiece that takes audiences on a thrilling and captivating journey. \n",
    "The breathtaking cinematography and impeccable special effects create a mesmerizing world that pulls you right into the heart of the story. \n",
    "The performances by the lead actors are commendable, with moments of raw and intense emotion that leave a lasting impact.\n",
    "\n",
    "However, the film's pacing at times feels uneven, with certain scenes dragging on a bit too long while others rush by, leaving you slightly disoriented. \n",
    "The intricate plot, while intriguing, can also become convoluted, requiring the audience to stay fully engaged to grasp all the nuances. \n",
    "Despite these minor drawbacks, \"A Rollercoaster of Emotions\" is an experience worth embarking upon for its highs that genuinely touch the soul, even though it occasionally loses its way on the journey.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d415e14-d681-4146-b49a-67ea1c4dfc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997828602790833}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(neutral_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f52aeb-8c0e-4cf6-9579-bf9c19af07db",
   "metadata": {},
   "source": [
    "The above review is more of neutral then also the model marked it as positive with higher probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e00b2b8d-4e12-4c4d-b5b7-c459e6548c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model='hipnologo/gpt2-imdb-finetune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21740f9a-5d36-4c26-960a-4592bb583c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9971871972084045}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(neutral_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21016a5c-fe47-4ae5-9fda-0af5eb7acae8",
   "metadata": {},
   "source": [
    "The `hipnologo/gpt2-imdb-finetune` model also classifies the above neutral text as positive with more probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9648a74-386c-434f-afcd-8ba151bba3a5",
   "metadata": {},
   "source": [
    "## Zero Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0e18e7-2fed-4135-8efc-9d0cf08b8533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39076623-9dcf-40c4-afb5-8f36bd1f9801",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_text = '''Amidst a fierce political debate, a last-minute goal secured victory in a sports showdown. \n",
    "Under the starlit sky, a new species emerged from the ocean's depths, as whispers of a forgotten melody lingered. \n",
    "Tokyo's vibrant streets witnessed a romantic encounter, while groundbreaking AI reshaped technology. \n",
    "In the Renaissance era, artists created masterpieces rivaling historic tales, and a decadent chocolate cake offered a symphony of flavors, \n",
    "a true culinary masterpiece'''\n",
    "candidate_labels = [\"Sports\", \"Science\", \"Romance\", \"Political\", \"Travel\", \"Technology\", \"Horror\", \"Culinary\", \"History\", \"Finance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dae60da-67a9-48f3-9c5b-1cd657077f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"Amidst a fierce political debate, a last-minute goal secured victory in a sports showdown. \\nUnder the starlit sky, a new species emerged from the ocean's depths, as whispers of a forgotten melody lingered. \\nTokyo's vibrant streets witnessed a romantic encounter, while groundbreaking AI reshaped technology. \\nIn the Renaissance era, artists created masterpieces rivaling historic tales, and a decadent chocolate cake offered a symphony of flavors, \\na true culinary masterpiece\",\n",
       " 'labels': ['Sports',\n",
       "  'Political',\n",
       "  'Romance',\n",
       "  'Culinary',\n",
       "  'Travel',\n",
       "  'History',\n",
       "  'Technology',\n",
       "  'Science',\n",
       "  'Finance',\n",
       "  'Horror'],\n",
       " 'scores': [0.17671708762645721,\n",
       "  0.16522802412509918,\n",
       "  0.1507391482591629,\n",
       "  0.13005705177783966,\n",
       "  0.1155952736735344,\n",
       "  0.09281495958566666,\n",
       "  0.07342732697725296,\n",
       "  0.04154600948095322,\n",
       "  0.034687407314777374,\n",
       "  0.01918768137693405]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroshot_classifier(candidate_text, candidate_labels=candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6583a-2ca9-47b7-8ad6-7db5c711d603",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be96cf08-93c6-46d3-9b16-767df2c77017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfc5b54e-71da-472a-911c-e6660a8298be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I wondered lonely as a cloud over me for a moment.\"\\n\\nI reached out to her and lifted her hand.\\n\\n\"Let me think'},\n",
       " {'generated_text': \"I wondered lonely as a cloud-haired wizard on my way to your city. You know I'd be here, waiting for you. This is my\"}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"I wondered lonely as a cloud\", num_return_sequences=2, max_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19568af0-5534-47ee-ba63-e0a7c7741e69",
   "metadata": {},
   "source": [
    "## Text Generation with Bengali Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6957da9f-3dce-48d9-a9b8-956310f8b813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "gen = pipeline('text-generation', model='ritog/bangla-gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2227f794-470e-4478-a777-2dfc1101d20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/anubhavmaity/mambaforge/envs/transformers-course/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'একা আমি ফিরব না আর হোসেন’জেমসের জন্য কাঁদলেন নোবেলজয়ী, পরিবারে শোকের ছায়া'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen('একা আমি ফিরব না আর')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563917f8-359a-4d69-a2a9-79f7381da2a5",
   "metadata": {},
   "source": [
    "## Mask Filing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048b531b-3b42-4166-b367-ea2215283c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e1743f9-cac7-422c-8bd6-b871ee3ca6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.24250896275043488,\n",
       "  'token': 20100,\n",
       "  'token_str': ' lonely',\n",
       "  'sequence': 'I walk a lonely road the only one that I have ever known'},\n",
       " {'score': 0.22409918904304504,\n",
       "  'token': 10667,\n",
       "  'token_str': ' dirt',\n",
       "  'sequence': 'I walk a dirt road the only one that I have ever known'},\n",
       " {'score': 0.06567858159542084,\n",
       "  'token': 26923,\n",
       "  'token_str': ' gravel',\n",
       "  'sequence': 'I walk a gravel road the only one that I have ever known'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('I walk a <mask> road the only one that I have ever known', top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05d8f8f3-9491-40aa-a75e-86cdb25645c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81822325-a999-42e6-9370-d20f398f1922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7492600679397583,\n",
       "  'token': 1109,\n",
       "  'token_str': 'The',\n",
       "  'sequence': 'The roads diverged in a yellow wood'},\n",
       " {'score': 0.032785188406705856,\n",
       "  'token': 2695,\n",
       "  'token_str': 'Both',\n",
       "  'sequence': 'Both roads diverged in a yellow wood'},\n",
       " {'score': 0.02911911904811859,\n",
       "  'token': 1103,\n",
       "  'token_str': 'the',\n",
       "  'sequence': 'the roads diverged in a yellow wood'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('[MASK] roads diverged in a yellow wood', top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5cf2f6-3e97-4c37-9436-ba62f0cb973d",
   "metadata": {},
   "source": [
    "Lets add the sentence `, And sorry I could not travel both` and see the output. With the added sentence I am giving more hint to the model that are two roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb827b1a-7e65-4e71-9064-30470c486763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6050049066543579,\n",
       "  'token': 1109,\n",
       "  'token_str': 'The',\n",
       "  'sequence': 'The roads diverged in a yellow wood, And sorry I could not travel both'},\n",
       " {'score': 0.13009047508239746,\n",
       "  'token': 3458,\n",
       "  'token_str': 'Our',\n",
       "  'sequence': 'Our roads diverged in a yellow wood, And sorry I could not travel both'},\n",
       " {'score': 0.02842896804213524,\n",
       "  'token': 1960,\n",
       "  'token_str': 'Two',\n",
       "  'sequence': 'Two roads diverged in a yellow wood, And sorry I could not travel both'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('[MASK] roads diverged in a yellow wood, And sorry I could not travel both', top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c48017-0030-4a22-85be-f8ce127e9cfa",
   "metadata": {},
   "source": [
    "The model guessed it as `Two` in 3rd prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b25766-6e45-4201-8f5b-82a6112a2d73",
   "metadata": {},
   "source": [
    "## NER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f9447-73bf-47df-bb2c-b835d4250b9f",
   "metadata": {},
   "source": [
    "Entities are \n",
    "1. Name \n",
    "2. Place\n",
    "3. Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c94a365c-c284-42c3-9d72-8862a6498404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/anubhavmaity/mambaforge/envs/transformers-course/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner', grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08d2d0b5-6b00-438b-8151-0a14f7f2dabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner('''After a long and wretched flight\n",
    "That stretched from daylight into night,\n",
    "Where babies wept and tempers shattered\n",
    "And the plane lurched and whiskey splattered\n",
    "Over my plastic food, I came\n",
    "To claim my bags from Baggage Claim\n",
    "\n",
    "Around, the carousel went around\n",
    "The anxious travelers sought and found\n",
    "Their bags, intact or gently battered,\n",
    "But to my foolish eyes what mattered\n",
    "Was a brave suitcase, red and small,\n",
    "That circled round, not mine at all.\n",
    "\n",
    "I knew that bag. It must be hers.\n",
    "We hadnt met in seven years!\n",
    "And as the metal plates squealed and clattered\n",
    "My happy memories chimed and chattered.\n",
    "An old man pulled it of the Claim.\n",
    "My bags appeared: I did the same. ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb976c2-5738-4fbd-b6a7-4f76aa3667ef",
   "metadata": {},
   "source": [
    "No entities returned as there are no entities belonging to the `name`, `place` and `organization` in the above poem written by Vikram Seth. Lets try a sentence with name & place in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6f1436c-9d60-46bb-a083-5db6a4f578dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9995754,\n",
       "  'word': 'Vikram Seth',\n",
       "  'start': 29,\n",
       "  'end': 40},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9977036,\n",
       "  'word': 'Kolkata',\n",
       "  'start': 58,\n",
       "  'end': 65}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner('The above poem is written by Vikram Seth, who was born in Kolkata')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4957353-fc8c-4e22-92b6-f14226d512f5",
   "metadata": {},
   "source": [
    "Below is the output with out grouping the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "093af6e5-657b-4192-b5db-4e5a2462c234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/anubhavmaity/mambaforge/envs/transformers-course/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner', grouped_entities=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11848bb6-de15-4a13-a7f9-b5c9bd20684f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99954695,\n",
       "  'index': 7,\n",
       "  'word': 'V',\n",
       "  'start': 29,\n",
       "  'end': 30},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9992286,\n",
       "  'index': 8,\n",
       "  'word': '##ik',\n",
       "  'start': 30,\n",
       "  'end': 32},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9996859,\n",
       "  'index': 9,\n",
       "  'word': '##ram',\n",
       "  'start': 32,\n",
       "  'end': 35},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9998404,\n",
       "  'index': 10,\n",
       "  'word': 'Seth',\n",
       "  'start': 36,\n",
       "  'end': 40},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9977036,\n",
       "  'index': 16,\n",
       "  'word': 'Kolkata',\n",
       "  'start': 58,\n",
       "  'end': 65}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner('The above poem is written by Vikram Seth, who was born in Kolkata')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0b3bd-2ecf-4f2f-b63f-aceb1ddb712b",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    ">  Assigns grammatical categories (like nouns, verbs, etc.) to words in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bac15af7-92c2-46eb-aebe-cdbc9920b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vblagoje/bert-english-uncased-finetuned-pos were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pos = pipeline(\"token-classification\", model=\"vblagoje/bert-english-uncased-finetuned-pos\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33f6cce0-a1b1-4587-81ff-55cb0fe97f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PRON',\n",
       "  'score': 0.9994246,\n",
       "  'word': 'my',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.9972132,\n",
       "  'word': 'name',\n",
       "  'start': 3,\n",
       "  'end': 7},\n",
       " {'entity_group': 'AUX',\n",
       "  'score': 0.9963749,\n",
       "  'word': 'is',\n",
       "  'start': 8,\n",
       "  'end': 10},\n",
       " {'entity_group': 'PROPN',\n",
       "  'score': 0.99478185,\n",
       "  'word': 'anubhav',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'CCONJ',\n",
       "  'score': 0.9991928,\n",
       "  'word': 'and',\n",
       "  'start': 19,\n",
       "  'end': 22},\n",
       " {'entity_group': 'PRON',\n",
       "  'score': 0.99943525,\n",
       "  'word': 'i',\n",
       "  'start': 23,\n",
       "  'end': 24},\n",
       " {'entity_group': 'AUX',\n",
       "  'score': 0.99505144,\n",
       "  'word': 'am',\n",
       "  'start': 25,\n",
       "  'end': 27},\n",
       " {'entity_group': 'ADP',\n",
       "  'score': 0.99937147,\n",
       "  'word': 'in',\n",
       "  'start': 28,\n",
       "  'end': 30},\n",
       " {'entity_group': 'PROPN',\n",
       "  'score': 0.99885774,\n",
       "  'word': 'seattle',\n",
       "  'start': 31,\n",
       "  'end': 38}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos('''My name is Anubhav and I am in Seattle''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b1e8ab94-71dd-4485-899b-19519393e801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ADP',\n",
       "  'score': 0.9977035,\n",
       "  'word': 'with',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.9994331,\n",
       "  'word': 'no',\n",
       "  'start': 5,\n",
       "  'end': 7},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.9990916,\n",
       "  'word': 'companion',\n",
       "  'start': 8,\n",
       "  'end': 17},\n",
       " {'entity_group': 'ADP',\n",
       "  'score': 0.9994623,\n",
       "  'word': 'to',\n",
       "  'start': 18,\n",
       "  'end': 20},\n",
       " {'entity_group': 'PRON',\n",
       "  'score': 0.9995864,\n",
       "  'word': 'my',\n",
       "  'start': 21,\n",
       "  'end': 23},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.99918777,\n",
       "  'word': 'mood',\n",
       "  'start': 24,\n",
       "  'end': 28},\n",
       " {'entity_group': 'PUNCT',\n",
       "  'score': 0.9996674,\n",
       "  'word': ',',\n",
       "  'start': 28,\n",
       "  'end': 29},\n",
       " {'entity_group': 'ADP',\n",
       "  'score': 0.99920064,\n",
       "  'word': 'against',\n",
       "  'start': 30,\n",
       "  'end': 37},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.99949336,\n",
       "  'word': 'the',\n",
       "  'start': 38,\n",
       "  'end': 41},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.9991099,\n",
       "  'word': 'wind',\n",
       "  'start': 42,\n",
       "  'end': 46},\n",
       " {'entity_group': 'SCONJ',\n",
       "  'score': 0.9976913,\n",
       "  'word': 'as',\n",
       "  'start': 47,\n",
       "  'end': 49},\n",
       " {'entity_group': 'PRON',\n",
       "  'score': 0.99947244,\n",
       "  'word': 'it',\n",
       "  'start': 50,\n",
       "  'end': 52},\n",
       " {'entity_group': 'AUX',\n",
       "  'score': 0.9980008,\n",
       "  'word': 'should',\n",
       "  'start': 53,\n",
       "  'end': 59},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': 0.8938708,\n",
       "  'word': 'be',\n",
       "  'start': 60,\n",
       "  'end': 62},\n",
       " {'entity_group': 'PUNCT',\n",
       "  'score': 0.99966705,\n",
       "  'word': ',',\n",
       "  'start': 62,\n",
       "  'end': 63},\n",
       " {'entity_group': 'PRON',\n",
       "  'score': 0.99955434,\n",
       "  'word': 'i',\n",
       "  'start': 64,\n",
       "  'end': 65},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': 0.9994306,\n",
       "  'word': 'walk',\n",
       "  'start': 66,\n",
       "  'end': 70},\n",
       " {'entity_group': 'PUNCT',\n",
       "  'score': 0.9996669,\n",
       "  'word': ',',\n",
       "  'start': 70,\n",
       "  'end': 71},\n",
       " {'entity_group': 'CCONJ',\n",
       "  'score': 0.99834514,\n",
       "  'word': 'but',\n",
       "  'start': 72,\n",
       "  'end': 75},\n",
       " {'entity_group': 'ADP',\n",
       "  'score': 0.99947065,\n",
       "  'word': 'in',\n",
       "  'start': 76,\n",
       "  'end': 78},\n",
       " {'entity_group': 'PRON',\n",
       "  'score': 0.99955314,\n",
       "  'word': 'my',\n",
       "  'start': 79,\n",
       "  'end': 81},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.99890316,\n",
       "  'word': 'solitude',\n",
       "  'start': 82,\n",
       "  'end': 90},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': 0.9992015,\n",
       "  'word': 'bow',\n",
       "  'start': 91,\n",
       "  'end': 94},\n",
       " {'entity_group': 'ADP',\n",
       "  'score': 0.9992987,\n",
       "  'word': 'to',\n",
       "  'start': 95,\n",
       "  'end': 97},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.99950826,\n",
       "  'word': 'the',\n",
       "  'start': 98,\n",
       "  'end': 101},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.99891794,\n",
       "  'word': 'wind',\n",
       "  'start': 102,\n",
       "  'end': 106},\n",
       " {'entity_group': 'PRON',\n",
       "  'score': 0.9991172,\n",
       "  'word': 'that',\n",
       "  'start': 107,\n",
       "  'end': 111},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': 0.9989425,\n",
       "  'word': 'buffets',\n",
       "  'start': 112,\n",
       "  'end': 119},\n",
       " {'entity_group': 'PRON',\n",
       "  'score': 0.9995585,\n",
       "  'word': 'me',\n",
       "  'start': 120,\n",
       "  'end': 122},\n",
       " {'entity_group': 'PUNCT',\n",
       "  'score': 0.99966645,\n",
       "  'word': '.',\n",
       "  'start': 122,\n",
       "  'end': 123}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos('''With no companion to my mood,\n",
    "Against the wind as it should be,\n",
    "I walk, but in my solitude\n",
    "Bow to the wind that buffets me.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b862cef-2366-4ca5-985f-7e9f5f9f993f",
   "metadata": {},
   "source": [
    "##  Question answering\n",
    "> Extracts information from the given context, but does not generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2ae6a04-0d12-4e04-8cfb-14ba2d6fe47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 473/473 [00:00<00:00, 45.3kB/s]\n",
      "Downloading model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 261M/261M [00:09<00:00, 27.1MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29.0/29.0 [00:00<00:00, 2.60kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 2.88MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 436k/436k [00:00<00:00, 3.69MB/s]\n"
     ]
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "abc5f029-fb3a-40a6-baab-ab05b4265b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.563761830329895, 'start': 215, 'end': 219, 'answer': 'dawn'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(question=\"When do I walk?\", context='''\n",
    "What can I say to you? How can I retract\n",
    "All that that fool my voice has spoken -\n",
    "Now that the facts are plain, the placid surface cracked,\n",
    "The protocols of friendship broken?\n",
    "I cannot walk by day as now I walk at dawn\n",
    "Past the still house where you lie sleeping.\n",
    "May the sun burn these footprints on the lawn\n",
    "And hold you in its warmth and keeping.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2957987d-0bd2-4f32-8270-f8910f12f477",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2b3cb3b-d4d0-4dbb-834b-e1fa002324b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.80k/1.80k [00:00<00:00, 135kB/s]\n",
      "Downloading pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.22G/1.22G [00:46<00:00, 26.3MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26.0/26.0 [00:00<00:00, 2.25kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 4.13MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 3.85MB/s]\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2d2a097-36c1-4a10-9a9b-8d042271f683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \" In the Schrödinger's cat paradox, a feline entity exists simultaneously, in a bewildering duality of being both alive and dead until observed . The burgeoning field of quantum computing, leveraging the intricate dance of qubits entangled through delicate quantum gates, \\xa0promises to revolutionize computation .\"}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer('''\n",
    "In the multidimensional labyrinth of quantum mechanics, \n",
    "where particles can exist in a superposition of states and become entangled across vast distances instantaneously, \n",
    "the very act of observation wields the power to collapse wavefunctions and determine measurable outcomes, \n",
    "challenging our intuitive understanding of reality. \n",
    "At the heart of this enigma lies the Schrödinger's cat paradox, \n",
    "an illustrative gedankenexperiment in which a feline entity exists simultaneously \n",
    "in a bewildering duality of being both alive and dead until observed, \n",
    "exposing the profound role of consciousness and measurement in the intricate tapestry of the quantum realm. \n",
    "The burgeoning field of quantum computing, \n",
    "leveraging the intricate dance of qubits entangled through delicate quantum gates, \n",
    "promises to revolutionize computation by harnessing the inherent parallelism and uncertainty of quantum states, \n",
    "potentially unraveling solutions to problems that were hitherto computationally intractable, \n",
    "and yet, it grapples with the imperfections of decoherence and error correction, \n",
    "mirroring the intricate dance of probability that underpins the very fabric of the quantum universe.\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07d06e6-75a4-46bc-9dce-43a517820138",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c690201e-1807-4d1e-99ee-3d641b899171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.33k/1.33k [00:00<00:00, 78.3kB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 302M/302M [00:11<00:00, 25.9MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 235/235 [00:00<00:00, 24.2kB/s]\n",
      "Downloading (…)olve/main/source.spm: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.06M/1.06M [00:00<00:00, 5.87MB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 813k/813k [00:00<00:00, 5.73MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.18M/2.18M [00:00<00:00, 6.11MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65.0/65.0 [00:00<00:00, 15.3kB/s]\n",
      "/Users/anubhavmaity/mambaforge/envs/transformers-course/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline('translation', model='salesken/translation-hi-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1cb6496a-940f-4486-a7fa-e9089cb96a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'The sweetness of happiness was so far, but this stranger came to the fore. The heart is in a strange state, without any reason. The sky is full of darkness, like a dark cry.'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator('''\n",
    "अब तक़ रही थी खुशियों की मिठास,\n",
    "पर आज आयी ये अजनबी उदासी।\n",
    "दिल में बसी है एक अजीब सी बेचैनी,\n",
    "बिना किसी वजह की, बिना किसी रास्ते।\n",
    "\n",
    "आसमान पर छाया है गहरा अँधेरा,\n",
    "जैसे मन में बसी है कोई गुहा।\n",
    "खो गई है सब वो हँसी की बुनाई,\n",
    "बन गए हैं तन्हाई के जाल में फसे।\n",
    "\n",
    "यादें लायी हैं ये दर्द की बूँदें,\n",
    "आँखों से बह रही हैं बेचैन आहें।\n",
    "मन में छाई है गहरी उदासी की रात,\n",
    "जैसे खो गया हो किसी सपने का सारा सफर।\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b7e2f-6108-4acb-820c-59a07f25bbeb",
   "metadata": {},
   "source": [
    "Taking the above text and passing to english to hindi text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29ce6eef-f8e1-4805-b5da-8591842ebee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.39k/1.39k [00:00<00:00, 144kB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 306M/306M [00:11<00:00, 26.8MB/s]\n",
      "Downloading (…)neration_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 293/293 [00:00<00:00, 26.4kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44.0/44.0 [00:00<00:00, 4.45kB/s]\n",
      "Downloading (…)olve/main/source.spm: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 812k/812k [00:00<00:00, 4.50MB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.07M/1.07M [00:01<00:00, 849kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.10M/2.10M [00:00<00:00, 6.43MB/s]\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline('translation', model='Helsinki-NLP/opus-mt-en-hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a58d0b64-bd5f-4c1e-8007-3e2e4fa072a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'और सुख का मीठा हाल तो दूर था, परन्तु वह परदेशी देखने को आया था, परन्तु मन पराया हुआ है; किसी कारण से नहीं, आकाश अन्धकार से भरा हुआ है, और घोर अन्धकार से भरा हुआ है।'}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator('''\n",
    "The sweetness of happiness was so far, but this stranger came to the fore. The heart is in a strange state, without any reason. The sky is full of darkness, like a dark cry.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9befff-bc5e-42aa-a68c-1bca55ecc0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-course",
   "language": "python",
   "name": "transformers-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
