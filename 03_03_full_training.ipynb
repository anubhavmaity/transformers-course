{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf4cdb0-b79f-48cc-a473-186211abdc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30084bd6-0576-4916-97d9-03efe068e4d4",
   "metadata": {},
   "source": [
    "# Full Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772c0cc-b981-46ed-bc92-1b26ebe1c184",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08e0062-90f4-4f14-aa6b-94c5387e99d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad5655a-237b-43de-818b-95e39bfe950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d053b1e8-6a9f-4141-96af-e7b14b8a9216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e169392c6df4d58adbfc6c2716eedf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d03104c0a1f47e5950e29b7b4384c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafbf44d165344808fe1132a5fe968c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba0a8f094ad4426aa6752b3b705744c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4144c6922a94a65a5cda317b2a943b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d94647c9e6945c3a1bca9b5329d2ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09983cc30a3f4262a9d8deab067d0774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"mrpc\"); dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20bd16f-470b-4048-805a-248573d5e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(set(dataset['test']['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f0056-faa0-4e55-8b44-05a84f889b80",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba63d465-85da-4144-8ae0-b50a42d0a7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88ebea2-5eb9-48fc-a0f9-ac268bc61d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf9addd-2662-4181-b64d-4322444fdc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(entity):\n",
    "    return tokenizer(entity['sentence1'], entity['sentence2'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ab0c83-5ad2-49da-9d09-3035d17db841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2b0ae2f0b44ea6b1a97e75190c5f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df18f260f810437182cb8ac69c0131cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da24bc75404f413caa105cef1ac48227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize, batched=True); tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab84943a-fc33-4b3f-b93d-ba1096220f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(['sentence1', 'sentence2', 'idx'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column('label', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57e60b17-1c20-4885-aa1a-eed5e0fd4f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9979ef-a18d-4f1e-b3a9-e63f75674331",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc38fd2f-46f5-47df-a013-32e699ca2d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a2621d1-e032-407f-8563-cc5b8dcf93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "760c02dc-afd6-458d-b762-3b109e1c378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23333b1e-4368-4b14-a0f5-69cb79d7c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(tokenized_dataset['train'], batch_size = 32, collate_fn = data_collator)\n",
    "valid_dl = DataLoader(tokenized_dataset['validation'], batch_size = 64, collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c427ed22-4d5f-400a-8c2b-15ab1665db39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([32]),\n",
       " 'input_ids': torch.Size([32, 79]),\n",
       " 'token_type_ids': torch.Size([32, 79]),\n",
       " 'attention_mask': torch.Size([32, 79])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dl:\n",
    "    batch = batch.to(device)\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcee94f1-36b6-4a2f-b917-fa8bf05f69ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2]),\n",
       " tensor(0.7167, device='cuda:0', grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**batch); outputs.logits.shape, outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1b98fd0-adb3-473c-badb-ba7c70b9b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eb1fafc-44f8-4082-a9c8-65365bb75a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9db5c58b-1a7c-49d8-99d2-72c7765b4b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e007cc0-0ac8-46c7-a385-adda5af84b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5030bab3-1153-4856-8d2b-540eff246c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60230a9b462740cc8e4cea982b042708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/lib/python3/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = num_labels).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dl:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2248085d-e0b0-48d5-96e6-32c56b8b4d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8455882352941176, 'f1': 0.887298747763864}\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for batch in valid_dl:\n",
    "    batch = batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    actuals = batch['labels']\n",
    "    metric.add_batch(predictions = predictions, references = actuals)\n",
    "print(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7205c774-da6e-44fa-9878-f8d7f9cc31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64df7f66-99d0-4551-89ba-bf7e42ead3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mrpc/tokenizer_config.json',\n",
       " 'mrpc/special_tokens_map.json',\n",
       " 'mrpc/vocab.txt',\n",
       " 'mrpc/added_tokens.json',\n",
       " 'mrpc/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb06a42-a87f-4030-85d7-5ddd95b290f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
